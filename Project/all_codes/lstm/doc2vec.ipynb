{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from __future__ import unicode_literals\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import spacy\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Embedding, LSTM, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#initiate sentences and labels lists\n",
    "sentences = []\n",
    "sentences_label = []\n",
    "\n",
    "#create sentences function:\n",
    "def create_sentences(doc):\n",
    "    ponctuation = [\"\\n\"]\n",
    "    sentences = []\n",
    "    sent = []\n",
    "    for word in doc:\n",
    "        if word.text not in ponctuation:\n",
    "            if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0'):\n",
    "                sent.append(word.text.lower())\n",
    "        else:\n",
    "            sent.append(word.text.lower())\n",
    "            if len(sent) > 1:\n",
    "                sentences.append(sent)\n",
    "            sent=[]\n",
    "    return sentences\n",
    "\n",
    "input_file = os.path.join(\"../inningsTXT/commentary.txt\")\n",
    "#read data\n",
    "with codecs.open(input_file, \"r\") as f:\n",
    "    data = unicode(f.read())\n",
    "#create sentences\n",
    "    doc = nlp(data)\n",
    "    sents = create_sentences(doc)\n",
    "    sentences = sentences + sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'no', u'runlength', u'on', u'off', u',', u'down', u'the', u'wicket', u'comes', u'kolhi', u'to', u'try', u'and', u'blast', u'him', u'through', u'the', u'covers', u',', u'only', u'gets', u'a', u'thick', u'inside', u'edge', u'near', u'his', u'feet', u'\\n']\n"
     ]
    }
   ],
   "source": [
    "print sentences[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  30 257  59  60  37   3 395 198  19   3 261   5  51  48  17 396  37 199\n",
      "   6  15   1 155  74  47  48  16  86 397 398   2]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=800, lower=True,split=' ')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "X = tokenizer.texts_to_sequences(sentences)\n",
    "X = pad_sequences(X)\n",
    "print X[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four runslength , drifting into the pads , and rahul , on return couldn' t have been happier . cashes in with another clip down to fine - leg \n"
     ]
    }
   ],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "# Function takes a tokenized sentence and returns the words\n",
    "def sequence_to_text(list_of_indices):\n",
    "    # Looking up words in dictionary\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    return(words)\n",
    "\n",
    "# Creating texts \n",
    "my_texts = list(map(sequence_to_text, X))\n",
    "\n",
    "for i in my_texts[11]:\n",
    "    if(i != None): print i,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(247, 84)\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0  54   0   0   0  76   0  84  23\n",
      "    0  45  25 106   0  90  51  70   7  56  61   0]]\n",
      "[u'point' u'fine' u'wicket' u'cover' u'run' u'runfull' u'wide'\n",
      " u'runquicker' u'over' u'gets' u'off' u'2' u'ball']\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# a = [0] * 84\n",
    "# for i in range(65, 84):\n",
    "#     a[i] = random.randint(0, 300)\n",
    "#     a[i] *= 1.1\n",
    "# a = np.array(a)\n",
    "# a = a.astype(np.int32)\n",
    "# a = a.reshape(1, 84)\n",
    "op_folder = \"./input\"\n",
    "Y = np.load(\"./results/lstm_pred.npy\")\n",
    "Y_pred = [int(max(0, int(round(Y[0][i])))) for i in range(len(Y[0]))]\n",
    "Y_pred = np.array(Y_pred)\n",
    "Y_pred = np.expand_dims(Y_pred, axis=0)\n",
    "print X.shape\n",
    "print Y_pred\n",
    "b = list(map(sequence_to_text, Y_pred))\n",
    "lstm_commentary = []\n",
    "for i in range(len(b)):\n",
    "    if(b[i] is not None):\n",
    "        lstm_commentary.append(b[i])\n",
    "lstm_commentary = np.array(lstm_commentary)\n",
    "final_comm = []\n",
    "for i in range(len(lstm_commentary[0])):\n",
    "    if(lstm_commentary[0][i] is not None):\n",
    "        final_comm.append(lstm_commentary[0][i])\n",
    "final_comm = np.array(final_comm)\n",
    "print(final_comm)\n",
    "np.save(\"./results/lstm_commentary.npy\", final_comm)\n",
    "print(len(final_comm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_dim = 128\n",
    "# lstm_out = 200\n",
    "# batch_size = 32\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(500, embed_dim,input_length = X.shape[1], dropout = 0.2))\n",
    "# model.add(LSTM(lstm_out, dropout_U = 0.2, dropout_W = 0.2))\n",
    "# model.add(Dense(2,activation='softmax'))\n",
    "# model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four runslength , drifting into the pads , and rahul , on return couldn' t have been happier . cashes in with another clip down to fine - leg \n"
     ]
    }
   ],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "# Function takes a tokenized sentence and returns the words\n",
    "def sequence_to_text(list_of_indices):\n",
    "    # Looking up words in dictionary\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    return(words)\n",
    "\n",
    "# Creating texts \n",
    "my_texts = list(map(sequence_to_text, X))\n",
    "\n",
    "for i in my_texts[11]:\n",
    "    if(i != None): print i,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "728\n"
     ]
    }
   ],
   "source": [
    "a = -1\n",
    "M = [max(a, max(X[i])) for i in range(len(X))]\n",
    "print max(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to: ./input\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(op_folder):\n",
    "    os.makedirs(op_folder)\n",
    "\n",
    "np.save(op_folder + \"/Y.npy\", X)\n",
    "print(\"Data saved to: \" + str(op_folder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virat\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "arr = [\"Virat\", \"Rahul\", \"Virat\", \"Rahul\", \"Zampa\", \"Virat\"]\n",
    "arr = np.array(arr)\n",
    "mode = stats.mode(arr)\n",
    "print mode[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdsds\n",
      "dsdd\n"
     ]
    }
   ],
   "source": [
    "s = \"sdsds\"\n",
    "s += \"\\n\"\n",
    "s += \"dsdd\"\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
